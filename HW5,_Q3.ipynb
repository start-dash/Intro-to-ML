{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMESN6vd0EtwTegAzgfQUKl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/start-dash/Intro-to-ML/blob/main/HW5%2C_Q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSbvHFumrZgL"
      },
      "outputs": [],
      "source": [
        "# Repeat all sections of problem 2 using all the input features from the housing price dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "# Create a classifier\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import cross_val_score\n",
        "#Creating an SVM classifier\n",
        "from sklearn import svm\n",
        "# \"Linear Support vector classifier\"\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA as RandomizedPCA\n",
        "\n",
        "#As an example of support vector machines in action, let's take a look at the facial recognition problem.\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from scipy.stats import mode\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# use seaborn plotting defaults\n",
        "import seaborn as sns; sns.set()\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "from ipywidgets import interact, fixed\n",
        "#let's look at some data that is not linearly separable:\n",
        "from sklearn.datasets import make_circles\n",
        "from mpl_toolkits import mplot3d\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "#Finally, we can use a grid search cross-validation to explore combinations of parameters.\n",
        "#Here we will adjust C (which controls the margin hardness)\n",
        "#We also explore gamma (which controls the size of the radial basis function kernel)\n",
        "#and determine the best model:\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# For better accuracy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "# Evaluate the model using evaluation metrics: Accuracy, precision, and recall.\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import SpectralClustering\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "8iAbZwcOrRr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read URL"
      ],
      "metadata": {
        "id": "7Vjcop6CgEov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/start-dash/Intro-to-ML/main/Datasets/Housing.csv\"\n",
        "housing = pd.read_csv(url)\n",
        "housing.head()"
      ],
      "metadata": {
        "id": "iXWMbjWwgBrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variables Mapping"
      ],
      "metadata": {
        "id": "4OfLwnPVgujs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "varlist = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
        "\n",
        "def binary_mapping(x):\n",
        "  return x.map({'no': 0, 'yes': 1})\n",
        "housing[varlist] = housing[varlist].apply(binary_mapping)\n",
        "housing = housing.drop('furnishingstatus', axis = 1)"
      ],
      "metadata": {
        "id": "KLUxBmfdgJHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = housing.pop('price')\n",
        "features = hf.columns.tolist()\n",
        "x = housing[varlist].values\n",
        "\n",
        "t_u = torch.tensor(x, dtype=torch.float32)\n",
        "t_un = torch.tensor(StandardScaler().fit_transform(x), dtype=torch.float32)\n",
        "t_c = torch.tensor(y, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "pj0VGmTAgjj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = t_un.shape[0]\n",
        "n_val = int(0.2 * n_samples)\n",
        "\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "\n",
        "train_indices = shuffled_indices[:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:]\n",
        "\n",
        "train_t_u = t_u[train_indices]\n",
        "train_t_un = t_un[train_indices]\n",
        "train_t_c = t_c[train_indices]\n",
        "\n",
        "val_t_u = t_u[val_indices]\n",
        "val_t_un = t_un[val_indices]\n",
        "val_t_c = t_c[val_indices]"
      ],
      "metadata": {
        "id": "ZAXekXmPgwXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lin_model(t_u, w11, w10, w9, w8, w7, w6, w5, w4, w3, w2, w1, b):\n",
        "  return torch.matmul(t_u, params[:-1]) + params[-1]\n",
        "\n",
        "def loss_fn(t_p, t_c):\n",
        "  squared_diffs = (t_p - t_c)**2\n",
        "  return squared_diffs.mean()\n",
        "\n",
        "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n",
        "  training_losses = []\n",
        "  valid_losses = []\n",
        "\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    train_t_p = lin_model(train_t_u, *params)\n",
        "    train_loss = loss_fn(train_t_p, train_t_c)\n",
        "\n",
        "    val_t_p = lin_model(val_t_u, *params)\n",
        "    val_loss = loss_fn(val_t_p, val_t_c)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    training_losses.append(train_loss)\n",
        "    valid_losses.append(val_loss)\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "      print(f'Epoch {epoch}, Training Loss {train_loss.item():.4f}, Validation Loss {val_loss.item():.4f}')\n",
        "  return params, train_loss, val_loss"
      ],
      "metadata": {
        "id": "v1-RtAl6hI93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SGD Modeling"
      ],
      "metadata": {
        "id": "SjAjve-ChSrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.sgd import SGD\n",
        "learning_rate = [0.1, 0.01, 0.001, 0.0001]\n",
        "for lr in learning_rate:\n",
        "  params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "  optimizerSGD_linear = optim.SGD([params], lr = lr)\n",
        "  print(f'Training with learning rate: {lr}')\n",
        "  training_loop(\n",
        "      n_epochs = 5000,\n",
        "      optimizer=optimizerSGD_linear,\n",
        "      params = params,\n",
        "      train_t_u = train_t_un,\n",
        "      val_t_u = val_t_un,\n",
        "      train_t_c = train_t_c,\n",
        "      val_t_c = val_t_c\n",
        "  )"
      ],
      "metadata": {
        "id": "5G6EOkB6hTjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store train and valid losses\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "optimizer = optim.SGD([params], lr = 0.1)\n",
        "\n",
        "model, trainLoss, validLoss = training_loop(5000, optimizer, params, train_t_un, val_t_un, train_t_c, val_t_c)\n",
        "SGD_trainLoss1 = np.array([item.detach().item() for item in trainLoss])\n",
        "SGD_validLoss1 = np.array([item.detach().item() for item in validLoss])\n",
        "\n",
        "optimizer = optim.SGD([params], lr = 0.01)\n",
        "\n",
        "model, trainLoss, validLoss = training_loop(5000, optimizer, params, train_t_un, val_t_un, train_t_c, val_t_c)\n",
        "SGD_trainLoss2 = np.array([item.detach().item() for item in trainLoss])\n",
        "SGD_validLoss2 = np.array([item.detach().item() for item in validLoss])\n",
        "\n",
        "optimizer = optim.SGD([params], lr = 0.001)\n",
        "\n",
        "model, trainLoss, validLoss = training_loop(5000, optimizer, params, train_t_un, val_t_un, train_t_c, val_t_c)\n",
        "SGD_trainLoss3 = np.array([item.detach().item() for item in trainLoss])\n",
        "SGD_validLoss3 = np.array([item.detach().item() for item in validLoss])\n",
        "\n",
        "optimizer = optim.SGD([params], lr = 0.0001)\n",
        "\n",
        "model, trainLoss, validLoss = training_loop(5000, optimizer, params, train_t_un, val_t_un, train_t_c, val_t_c)\n",
        "SGD_trainLoss4 = np.array([item.detach().item() for item in trainLoss])\n",
        "SGD_validLoss4 = np.array([item.detach().item() for item in validLoss])"
      ],
      "metadata": {
        "id": "6QC7lev8jCKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam Modeling"
      ],
      "metadata": {
        "id": "gd9YVpudhYlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = [0.1, 0.01, 0.001, 0.0001]\n",
        "for lr in learning_rate:\n",
        "  params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "  optimizerADAM_linear = optim.Adam([params], lr = lr)\n",
        "  print(f'Training with learning rate: {lr}')\n",
        "  training_loop(\n",
        "        n_epochs = 5000,\n",
        "        optimizer=optimizerADAM_linear,\n",
        "        params = params,\n",
        "        train_t_u = train_t_un,\n",
        "        val_t_u = val_t_un,\n",
        "        train_t_c = train_t_c,\n",
        "        val_t_c = val_t_c\n",
        "    )"
      ],
      "metadata": {
        "id": "Lp_4ZOSmhZch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store train and valid losses\n",
        "optimizer = optim.Adam([params], lr = 0.1)\n",
        "\n",
        "model, trainLoss, validLoss = training_loop(5000, optimizer, params, train_t_un, val_t_un, train_t_c, val_t_c)\n",
        "ADAM_trainLoss1 = np.array([item.detach().item() for item in trainLoss])\n",
        "ADAM_validLoss1 = np.array([item.detach().item() for item in validLoss])\n",
        "\n",
        "optimizer = optim.Adam([params], lr = 0.01)\n",
        "\n",
        "model, trainLoss, validLoss = training_loop(5000, optimizer, params, train_t_un, val_t_un, train_t_c, val_t_c)\n",
        "ADAM_trainLoss2 = np.array([item.detach().item() for item in trainLoss])\n",
        "ADAM_validLoss2 = np.array([item.detach().item() for item in validLoss])\n",
        "\n",
        "optimizer = optim.Adam([params], lr = 0.001)\n",
        "\n",
        "model, trainLoss, validLoss = training_loop(5000, optimizer, params, train_t_un, val_t_un, train_t_c, val_t_c)\n",
        "ADAM_trainLoss3 = np.array([item.detach().item() for item in trainLoss])\n",
        "ADAM_validLoss3 = np.array([item.detach().item() for item in validLoss])\n",
        "\n",
        "optimizer = optim.Adam([params], lr = 0.0001)\n",
        "\n",
        "model, trainLoss, validLoss = training_loop(5000, optimizer, params, train_t_un, val_t_un, train_t_c, val_t_c)\n",
        "ADAM_trainLoss4 = np.array([item.detach().item() for item in trainLoss])\n",
        "ADAM_validLoss4 = np.array([item.detach().item() for item in validLoss])"
      ],
      "metadata": {
        "id": "1zsbdQiujBw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression Loss Plots"
      ],
      "metadata": {
        "id": "6wgOEZu1jG_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD Optimized\n",
        "plt.figure(figsize = (10, 10))\n",
        "plt.suptitle(\"SGD Optimized Linear Regression Loss w/ 5 Inputs\")\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "plt.plot(SGD_trainLoss1, color='black')\n",
        "plt.plot(SGD_validLoss1, color='red')\n",
        "plt.title('Loss w/ Learning Rate of 0.1')\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.plot(SGD_trainLoss2, color='black')\n",
        "plt.plot(SGD_validLoss2, color='red')\n",
        "plt.title('Loss w/ Learning Rate of 0.01')\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "plt.plot(SGD_trainLoss3, color='black')\n",
        "plt.plot(SGD_validLoss3, color='red')\n",
        "plt.title('Loss w/ Learning Rate of 0.001')\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "plt.plot(SGD_trainLoss3, color='black')\n",
        "plt.plot(SGD_validLoss3, color='red')\n",
        "plt.title('Loss w/ Learning Rate of 0.0001')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "owoYJE0DjJ_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam Optimized\n",
        "plt.figure(figsize = (10, 10))\n",
        "plt.suptitle(\"Adam Optimized Linear Regression Loss w/ 5 Inputs\")\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "plt.plot(ADAM_trainLoss1, color='black')\n",
        "plt.plot(ADAM_validLoss1, color='red')\n",
        "plt.title('Loss w/ Learning Rate of 0.1')\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.plot(ADAM_trainLoss2, color='black')\n",
        "plt.plot(ADAM_validLoss2, color='red')\n",
        "plt.title('Loss w/ Learning Rate of 0.01')\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "plt.plot(ADAM_trainLoss3, color='black')\n",
        "plt.plot(ADAM_validLoss3, color='red')\n",
        "plt.title('Loss w/ Learning Rate of 0.001')\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "plt.plot(ADAM_trainLoss3, color='black')\n",
        "plt.plot(ADAM_validLoss3, color='red')\n",
        "plt.title('Loss w/ Learning Rate of 0.0001')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IB6vFdN7jL8l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}